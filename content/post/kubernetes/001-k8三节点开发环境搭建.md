---
title:          001-K8S三节点开发环境搭建
date:           2020-01-23T14:20:23+08:00
draft:          true
tags:           [2020-01]
topics:         [容器,kubernetes]
---


## 简介

&nbsp;&nbsp;&nbsp;&nbsp;由于工作的原因，需要对k8s进行二次开发，所以需要一个能统一的开发环境，而minikube并不适合深度开发，所以只能以最小集群的方式搭建开发环境。主要是宿主机使用vagrant+virtualbox通过Vagrantfile文件快速产生半成品虚拟主机集群，然后通过简单配置即可产生k8s三节点开发环境。本次有使用代理方式安装，请处理你是否熟悉代理。

## 环境

&nbsp;&nbsp;&nbsp;&nbsp;虚拟机配置（最低）：

{{< pure_table
"IP|HostName|CPU|Memory|NodeType"
"172.20.0.201|KNode1|1|2G|master"
"172.20.0.202|KNode2|1|2G|node"
"172.20.0.203|KNode3|1|2G|node"
>}}

&nbsp;&nbsp;&nbsp;&nbsp;宿主机配置（最低）：

{{< pure_table
"CPU|Memory|software"
"4|8G|vagrant virtualbox"
>}}

vagrantfile文件内容：

    Vagrant.configure("2") do |config|

        (1..3).each do |i|

            config.vm.define "KNode#{i}" do |node|

            # 设置虚拟机的Box
            node.vm.box = "bento/centos-7.6"

            # 设置虚拟机的主机名
            node.vm.hostname="KNode#{i}"

            # 设置虚拟机的IP
            node.vm.network "private_network", ip: "172.20.0.20#{i}"

            # 设置主机与虚拟机的共享目录
            #node.vm.synced_folder "/Users/temp/Development/vagrant/development", "/vagrant/share"

            # VirtaulBox相关配置
            node.vm.provider "virtualbox" do |v|

                # 设置虚拟机的名称
                v.name = "KNode#{i}"

                # 设置虚拟机的内存大小  
                v.memory = 2048

                # 设置虚拟机的CPU个数
                v.cpus = 2
            end
        # 设置网络代理(需要安装 vagrant-proxyconf 插件)
            #if Vagrant.has_plugin?("vagrant-proxyconf")
        # 若安装了plugin，则设置代理信息
        #  config.proxy.http     = "http://172.20.0.1:8000/"
        #  config.proxy.https    = "http://172.20.0.1:8000/"
        #  config.proxy.no_proxy = "localhost,127.0.0.1,.example.com"
        #else
        # 若没有安装plugin，则调用系统命令安装插件，并提示重运行命令
        #	system('vagrant plugin install vagrant-proxyconf')
        #	raise("vagrant-proxyconf installed. Run command again.");
        #end
            # 使用shell脚本进行软件安装和配置
            node.vm.provision "shell", inline: <<-SHELL
                yum --exclude=kernel* update -y
                yum install epel-release -y
                yum install bash-com* net-tools sysstat vim wget telnet privoxy -y
                yum remove docker docker-client docker-client-latest docker-common docker-latest docker-latest-logrotate docker-logrotate docker-engine  -y
                yum install -y yum-utils device-mapper-persistent-data lvm2 -y
                yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo -y
                yum install docker-ce -y
                usermod -aG docker vagrant
                swapoff -a && sysctl -w vm.swappiness=0 && systemctl stop firewalld && systemctl disable firewalld && setenforce 0 && sed -i 's/SELINUX=permissive/SELINUX=disabled/' /etc/selinux/config && sed -i '/swap/s/^/#/' /etc/fstab
                curl "https://bootstrap.pypa.io/get-pip.py" -o "get-pip.py"
                python get-pip.py
                pip install shadowsocks
                yum install libsodium -y
                pip install https://github.com/shadowsocks/shadowsocks/archive/master.zip -U
                cat > /etc/sysconfig/modules/ipvs.modules <<EOF
    #!/bin/bash
    modprobe -- br_netfilter
    modprobe -- ip_vs
    modprobe -- ip_vs_rr
    modprobe -- ip_vs_wrr
    modprobe -- ip_vs_sh
    modprobe -- nf_conntrack_ipv4
    EOF
                cat <<EOF > /etc/sysctl.d/k8s.conf
    net.ipv4.ip_forward = 1
    net.bridge.bridge-nf-call-ip6tables = 1
    net.bridge.bridge-nf-call-iptables = 1
    EOF

                chmod 755 /etc/sysconfig/modules/ipvs.modules && \
                bash /etc/sysconfig/modules/ipvs.modules && \
                lsmod | grep -E "ip_vs|nf_conntrack_ipv4" && sysctl -p /etc/sysctl.d/k8s.conf
                cat <<EOF > /etc/shadowsocks.json
    {
    "server": "代理地址",
    "server_port": 代理端口,
    "local_address": "127.0.0.1",
    "local_port":1080,
    "password": "代理密码",
    "timeout":300,
    "method": "加密方式",
    "workers": 1
    }
    EOF
                cat <<EOF > /etc/systemd/system/shadowsocks-local.service
    [Unit]
    Description=Shadowsocks
    [Service]
    TimeoutStartSec=0
    ExecStart=/usr/bin/sslocal -c /etc/shadowsocks.json
    [Install]
    WantedBy=multi-user.target
    EOF
                cat <<EOF >> /etc/hosts
    172.20.0.201 KNode1 KNode1.example.com
    172.20.0.202 KNode2 KNode2.example.com
    172.20.0.203 KNode3 KNode3.example.com
    EOF
                echo "forward-socks5t   /               127.0.0.1:1080 ." >> /etc/privoxy/config
                systemctl enable shadowsocks-local && systemctl start shadowsocks-local && systemctl enable privoxy.service && systemctl start privoxy.service
                sed -i 's@ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock@ExecStart=/usr/bin/dockerd -H fd:// -H tcp://0.0.0.0:2375 --containerd=/run/containerd/containerd.sock @' /lib/systemd/system/docker.service
                sed -i '/Type=notify/a\Environment="HTTP_PROXY=http://127.0.0.1:8118/" "HTTPS_PROXY=http://127.0.0.1:8118" "NO_PROXY=localhost,127.0.0.1,fsw1qay4.mirror.aliyuncs.com"' /lib/systemd/system/docker.service
                systemctl enable docker && systemctl start docker
                touch /etc/docker/daemon.json
                cat > /etc/docker/daemon.json << EOF
    {
        "exec-opts": ["native.cgroupdriver=systemd"],
        "log-driver": "json-file",
        "log-opts": {
            "max-size": "100m",
            "max-file": "10"
        },
        "oom-score-adjust": -1000,
        "storage-driver": "overlay2",
        "storage-opts":["overlay2.override_kernel_check=true"],
        "live-restore": false,
        "max-concurrent-downloads": 20,
        "data-root": "/opt/docker",
        "exec-root": "/opt/docker",
        "bridge": "none",
        "debug": true,
        "default-ulimits": {
            "nofile": {
                "Name": "nofile",
                "Hard": 1024000,
                "Soft": 1024000
            },
            "nproc": {
                "Name": "nproc",
                "Hard": 1024000,
                "Soft": 1024000
            },
        "core": {
                "Name": "core",
                "Hard": -1,
                "Soft": -1    
        }

        }
    }
    EOF
                systemctl restart docker
            SHELL

            end
        end
    end


## 软件包

无

## 拓扑图

无

## 正文
---
### 初始化集群

#### 1.配置集群TLS证书（KNode1节点）

&nbsp;&nbsp;&nbsp;&nbsp;这里使用独立的etcd集群，所以需要配置tls证书，使用cfssl工具快速生成证书。

##### 1).安装配置CFSSL工具


    export CFSSL_URL="https://pkg.cfssl.org/R1.2"
    wget "${CFSSL_URL}/cfssl_linux-amd64" -O /usr/bin/cfssl
    wget "${CFSSL_URL}/cfssljson_linux-amd64" -O /usr/bin/cfssljson
    wget "${CFSSL_URL}/cfssl-certinfo_linux-amd64" -O /usr/bin/cfssl-certinfo
    chmod +x /usr/bin/cfssl /usr/bin/cfssljson /usr/bin/cfssl-certinfo && mkdir -p /etc/etcd/ssl && cd /etc/etcd/ssl

##### 2).配置生成证书

    cat <<EOF > etcd-ca-csr.json
    {"CN":"etcd","key":{"algo":"rsa","size":2048},"names":[{"C":"CN","ST":"Hubei","L":"Wuhan","O":"etcd","OU":"Etcd Security"}]}
    EOF

    cat <<EOF > ca-config.json
    {"signing":{"default":{"expiry":"87600h"},"profiles":{"kubernetes":{"usages":["signing","key encipherment","server auth","client auth"],"expiry":"87600h"}}}}
    EOF

    cfssl gencert -initca etcd-ca-csr.json | cfssljson -bare etcd-ca

    cat <<EOF > etcd-csr.json
    {"CN":"etcd","hosts":["127.0.0.1","172.20.0.201","172.20.0.202","172.20.0.203"],"key":{"algo":"rsa","size":2048},"names":[{"C":"CN","ST":"Hubei","L":"Wuhan","O":"etcd","OU":"Etcd Security"}]}
    EOF

    cfssl gencert \
    -ca=etcd-ca.pem \
    -ca-key=etcd-ca-key.pem \
    -config=ca-config.json \
    -profile=kubernetes \
    etcd-csr.json | cfssljson -bare etcd

#### 2.配置etcd集群

##### 1).KNode1节点

    cd /vagrant && wget https://github.com/etcd-io/etcd/releases/download/v3.3.18/etcd-v3.3.18-linux-amd64.tar.gz
    tar xf /vagrant/etcd-v3.3.18-linux-amd64.tar.gz && mv etcd-v3.3.18-linux-amd64/etcd* /usr/bin/ && rm -rf etcd-v3.3.18-linux-amd64
    cp /etc/etcd /vagrant -rf

    cat <<EOF >/etc/etcd/etcd.conf
    # [member]
    ETCD_NAME=KNode1
    ETCD_DATA_DIR=/opt/etcd
    ETCD_LISTEN_PEER_URLS=https://0.0.0.0:2380
    ETCD_LISTEN_CLIENT_URLS=https://0.0.0.0:2379
    ETCD_PROXY=off
    # [cluster]
    ETCD_ADVERTISE_CLIENT_URLS=https://172.20.0.201:2379
    ETCD_INITIAL_ADVERTISE_PEER_URLS=https://172.20.0.201:2380
    ETCD_INITIAL_CLUSTER="KNode1=https://172.20.0.201:2380,KNode2=https://172.20.0.202:2380,KNode3=https://172.20.0.203:2380"
    ETCD_INITIAL_CLUSTER_STATE=new
    ETCD_INITIAL_CLUSTER_TOKEN=etcd-k8s-cluster
    # [security]
    ETCD_CERT_FILE="/etc/etcd/ssl/etcd.pem"
    ETCD_KEY_FILE="/etc/etcd/ssl/etcd-key.pem"
    ETCD_CLIENT_CERT_AUTH="true"
    ETCD_TRUSTED_CA_FILE="/etc/etcd/ssl/etcd-ca.pem"
    ETCD_AUTO_TLS="true"
    ETCD_PEER_CERT_FILE="/etc/etcd/ssl/etcd.pem"
    ETCD_PEER_KEY_FILE="/etc/etcd/ssl/etcd-key.pem"
    ETCD_PEER_CLIENT_CERT_AUTH="true"
    ETCD_PEER_TRUSTED_CA_FILE="/etc/etcd/ssl/etcd-ca.pem"
    ETCD_PEER_AUTO_TLS="true"
    EOF

##### 2).KNode2节点

    mkdir /etc/etcd/ssl/ -p && cd /etc/etcd/ssl/ && cp /vagrant/etcd/ssl/{etcd.pem,etcd-key.pem,etcd-ca.pem} .
    tar xf /vagrant/etcd-v3.3.18-linux-amd64.tar.gz && mv etcd-v3.3.18-linux-amd64/etcd* /usr/bin/ && rm -rf etcd-v3.3.18-linux-amd64
    cat <<EOF >/etc/etcd/etcd.conf
    # [member]
    ETCD_NAME=KNode2
    ETCD_DATA_DIR=/opt/etcd
    ETCD_LISTEN_PEER_URLS=https://0.0.0.0:2380
    ETCD_LISTEN_CLIENT_URLS=https://0.0.0.0:2379
    ETCD_PROXY=off
    # [cluster]
    ETCD_ADVERTISE_CLIENT_URLS=https://172.20.0.202:2379
    ETCD_INITIAL_ADVERTISE_PEER_URLS=https://172.20.0.202:2380
    ETCD_INITIAL_CLUSTER="KNode1=https://172.20.0.201:2380,KNode2=https://172.20.0.202:2380,KNode3=https://172.20.0.203:2380"
    ETCD_INITIAL_CLUSTER_STATE=new
    ETCD_INITIAL_CLUSTER_TOKEN=etcd-k8s-cluster
    # [security]
    ETCD_CERT_FILE="/etc/etcd/ssl/etcd.pem"
    ETCD_KEY_FILE="/etc/etcd/ssl/etcd-key.pem"
    ETCD_CLIENT_CERT_AUTH="true"
    ETCD_TRUSTED_CA_FILE="/etc/etcd/ssl/etcd-ca.pem"
    ETCD_AUTO_TLS="true"
    ETCD_PEER_CERT_FILE="/etc/etcd/ssl/etcd.pem"
    ETCD_PEER_KEY_FILE="/etc/etcd/ssl/etcd-key.pem"
    ETCD_PEER_CLIENT_CERT_AUTH="true"
    ETCD_PEER_TRUSTED_CA_FILE="/etc/etcd/ssl/etcd-ca.pem"
    ETCD_PEER_AUTO_TLS="true"
    EOF

##### 3).KNode3节点

    mkdir /etc/etcd/ssl/ -p && cd /etc/etcd/ssl/ && cp /vagrant/etcd/ssl/{etcd.pem,etcd-key.pem,etcd-ca.pem} . 
    tar xf /vagrant/etcd-v3.3.18-linux-amd64.tar.gz && mv etcd-v3.3.18-linux-amd64/etcd* /usr/bin/ && rm -rf etcd-v3.3.18-linux-amd64

    cat <<EOF >/etc/etcd/etcd.conf
    # [member]
    ETCD_NAME=KNode3
    ETCD_DATA_DIR=/opt/etcd
    ETCD_LISTEN_PEER_URLS=https://0.0.0.0:2380
    ETCD_LISTEN_CLIENT_URLS=https://0.0.0.0:2379
    ETCD_PROXY=off
    # [cluster]
    ETCD_ADVERTISE_CLIENT_URLS=https://172.20.0.203:2379
    ETCD_INITIAL_ADVERTISE_PEER_URLS=https://172.20.0.203:2380
    ETCD_INITIAL_CLUSTER="KNode1=https://172.20.0.201:2380,KNode2=https://172.20.0.202:2380,KNode3=https://172.20.0.203:2380"
    ETCD_INITIAL_CLUSTER_STATE=new
    ETCD_INITIAL_CLUSTER_TOKEN=etcd-k8s-cluster
    # [security]
    ETCD_CERT_FILE="/etc/etcd/ssl/etcd.pem"
    ETCD_KEY_FILE="/etc/etcd/ssl/etcd-key.pem"
    ETCD_CLIENT_CERT_AUTH="true"
    ETCD_TRUSTED_CA_FILE="/etc/etcd/ssl/etcd-ca.pem"
    ETCD_AUTO_TLS="true"
    ETCD_PEER_CERT_FILE="/etc/etcd/ssl/etcd.pem"
    ETCD_PEER_KEY_FILE="/etc/etcd/ssl/etcd-key.pem"
    ETCD_PEER_CLIENT_CERT_AUTH="true"
    ETCD_PEER_TRUSTED_CA_FILE="/etc/etcd/ssl/etcd-ca.pem"
    ETCD_PEER_AUTO_TLS="true"
    EOF


##### 4).配置并启动etcd集群（全节点）

    cat <<EOF > /lib/systemd/system/etcd.service
    [Unit]
    Description=Etcd Service
    After=network.target
    [Service]
    Environment=ETCD_DATA_DIR=/var/lib/etcd/default
    EnvironmentFile=-/etc/etcd/etcd.conf
    Type=notify
    User=etcd
    PermissionsStartOnly=true
    ExecStart=/usr/bin/etcd
    Restart=on-failure
    RestartSec=10
    LimitNOFILE=65536
    [Install]
    WantedBy=multi-user.target
    EOF

    groupadd etcd && useradd -c "Etcd user" -g etcd -s /sbin/nologin -r etcd && mkdir -p /opt/etcd && chown etcd:etcd -R /opt/etcd /etc/etcd
    systemctl enable etcd.service && systemctl start etcd.service

##### 5).验证

    etcdctl --ca-file /etc/etcd/ssl/etcd-ca.pem --cert-file /etc/etcd/ssl/etcd.pem --key-file /etc/etcd/ssl/etcd-key.pem --endpoints=https://172.20.0.201:2379,https://172.20.0.202:2379,https://172.20.0.203:2379 member list

#### 3.初始化K8S集群

##### 1). 安装配置master节点（KNode1节点）
    cat <<EOF > /etc/yum.repos.d/kubernetes.repo
    [kubernetes]
    name=Kubernetes
    baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
    enabled=1
    gpgcheck=1
    repo_gpgcheck=1
    gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
    EOF

    echo "proxy=http://127.0.0.1:8118" >> /etc/yum.conf
    yum install -y kubelet kubeadm kubectl
    systemctl enable kubelet && sudo systemctl start kubelet

    打印默认配置
    kubeadm config print init-defaults

    kubeadm token generate
    p7x8lk.71grkqn9q0o8nidk

    cat <<EOF > /vagrant/kubeadm.conf
    apiVersion: kubeadm.k8s.io/v1beta2
    bootstrapTokens:
    - groups:
      - system:bootstrappers:kubeadm:default-node-token
      token: p7x8lk.71grkqn9q0o8nidk
      ttl: 24h0m0s
      usages:
      - signing
      - authentication
    kind: InitConfiguration
    localAPIEndpoint:
      advertiseAddress: 172.20.0.201
      bindPort: 6443
    nodeRegistration:
      criSocket: /var/run/dockershim.sock
      name: KNode1
      taints:
      - effect: NoSchedule
        key: node-role.kubernetes.io/master
    ---
    apiServer:
      timeoutForControlPlane: 4m0s
    apiVersion: kubeadm.k8s.io/v1beta2
    certificatesDir: /etc/kubernetes/pki
    clusterName: kubernetes
    controllerManager: {}
    dns:
      type: CoreDNS
    etcd:
      external:
        endpoints:
        - https://172.20.0.201:2379
        - https://172.20.0.202:2379
        - https://172.20.0.203:2379
        caFile: /etc/etcd/ssl/etcd-ca.pem
        certFile: /etc/etcd/ssl/etcd.pem
        keyFile: /etc/etcd/ssl/etcd-key.pem
    imageRepository: k8s.gcr.io
    kind: ClusterConfiguration
    kubernetesVersion: v1.18.0
    networking:
      dnsDomain: allposs.local
      serviceSubnet: 10.96.0.0/12
    scheduler: {}
    EOF

    kubeadm init --config kubeadm.conf
完成后提示：kubeadm join --token b99a00.a144ef80536d4344 172.20.0.111:6443 --discovery-token-ca-cert-hash sha256:2a6a2f012022ce3ba5370efed1d2879942ea28dae1fa979651080f6b5e44ff63，请记录下。

    export KUBECONFIG=/etc/kubernetes/admin.conf

    echo "export KUBECONFIG=/etc/kubernetes/admin.conf" >> ~/.bash_profile

##### 2). 配置calico网络（KNode1节点）

    curl https://docs.projectcalico.org/v3.10/manifests/calico-etcd.yaml -o calico.yaml
    sed -i 's@.*etcd_endpoints:.*@\ \ etcd_endpoints:\ \"https://172.20.0.201:2379,https://172.20.0.202:2379,https://172.20.0.203:2379\"@gi' calico.yaml
    export ETCD_CERT=`cat /etc/etcd/ssl/etcd.pem | base64 | tr -d '\n'`
    export ETCD_KEY=`cat /etc/etcd/ssl/etcd-key.pem | base64 | tr -d '\n'`
    export ETCD_CA=`cat /etc/etcd/ssl/etcd-ca.pem | base64 | tr -d '\n'`


    sed -i "s@.*etcd-cert:.*@\ \ etcd-cert:\ ${ETCD_CERT}@gi" calico.yaml
    sed -i "s@.*etcd-key:.*@\ \ etcd-key:\ ${ETCD_KEY}@gi" calico.yaml
    sed -i "s@.*etcd-ca:.*@\ \ etcd-ca:\ ${ETCD_CA}@gi" calico.yaml

    sed -i 's@.*etcd_ca:.*@\ \ etcd_ca:\ "/calico-secrets/etcd-ca"@gi' calico.yaml
    sed -i 's@.*etcd_cert:.*@\ \ etcd_cert:\ "/calico-secrets/etcd-cert"@gi' calico.yaml
    sed -i 's@.*etcd_key:.*@\ \ etcd_key:\ "/calico-secrets/etcd-key"@gi' calico.yaml

    sed -i 's@192.168.0.0/16@10.85.0.0/16@gi' calico.yaml

    kubectl create -f calico.yaml

#### 4. 配置安装Node节点
   
    cat <<EOF > /etc/yum.repos.d/kubernetes.repo
    [kubernetes]
    name=Kubernetes
    baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
    enabled=1
    gpgcheck=1
    repo_gpgcheck=1
    gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
    EOF
    echo "proxy=http://127.0.0.1:8118" >> /etc/yum.conf
    yum install -y kubelet kubeadm
    systemctl enable kubelet && sudo systemctl start kubelet
    kubeadm join --token b99a00.a144ef80536d4344 172.20.0.111:6443 --discovery-token-ca-cert-hash sha256:2a6a2f012022ce3ba5370efed1d2879942ea28dae1fa979651080f6b5e44ff63

---
## 结束

链接：

1> https://segmentfault.com/a/1190000006917884